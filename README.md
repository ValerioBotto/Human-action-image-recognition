# Multi-Layer Perceptron per la Classificazione Multimodale

> Progetto finale per l‚Äôesame di **Intelligenza Artificiale**  
> Corso di laurea magistrale LM-43 - Universit√† di Catania  
> Anno accademico 2024/2025

---

## üß† Obiettivo del progetto

Il progetto mira a **classificare immagini di azioni umane** sfruttando **tecniche di classificazione multimodale**, ovvero combinando l'informazione visuale (immagine) e quella testuale (descrizione). Il modello √® addestrato a riconoscere l'azione svolta (es. "dancing", "eating", "sleeping"...) osservando sia l‚Äôimmagine che una descrizione coerente.

---

## üìÇ Dataset utilizzato

√à stato utilizzato un dataset annotato (https://huggingface.co/datasets/Bingsu/Human_Action_Recognition) contenente:

- Immagini raffiguranti 15 classi di azioni umane
- Descrizioni testuali sintetiche per ogni immagine (es. `a photo of a person smiling with an open mouth`)

Ogni immagine √® associata a una label e a una frase coerente per abilitare la classificazione multimodale tramite modello CLIP.

---

## üìå Task affrontato

Il task √® un **multi-class classification**, cio√® assegnare ad ogni immagine una **sola classe** tra 15 possibili azioni. Si tratta di un **problema supervisionato**, dove le etichette vere sono note.

---

## üß© Come viene affrontato

1. **Preprocessing**:
   - Le immagini sono preprocessate in formato compatibile con CLIP
   - Il testo √® tokenizzato e normalizzato
2. **Feature Extraction**:
   - Vengono estratti **embedding visivi** e **embedding testuali** usando il modello CLIP
   - I due embedding vengono **concatenati** per ottenere una rappresentazione multimodale di ogni esempio
3. **Classificazione**:
   - La rappresentazione multimodale viene passata a un **MLP** (Multi-Layer Perceptron) per predire la classe
   - Ottimizzazione con **SGD**, loss = cross entropy

---

## ü§ñ Modello utilizzato: CLIP + MLP

### üîç Cos'√® CLIP?

[CLIP](https://huggingface.co/openai/clip-vit-base-patch32) (Contrastive Language-Image Pretraining) √® un **modello multimodale pre-addestrato** da OpenAI. Il suo obiettivo √® mappare **immagini** e **testi** nello stesso spazio vettoriale in modo che descrizioni testuali corrette siano **vicine** alle immagini corrispondenti.

- Encoder immagine: `ViT-B/32` (Vision Transformer)
- Encoder testo: Transformer classico

CLIP viene usato **come estrattore di feature**, e non viene riaddestrato. Le feature vengono poi usate per la classificazione.

---

## üß™ Tecniche utilizzate

- **Classificazione multimodale** (immagine + testo)
- **Feature fusion** via concatenazione
- **MLP con ReLU** come classificatore
- **Ottimizzazione** con Stochastic Gradient Descent
- **Early stopping** 
- **Valutazione** con Accuracy e Confusion Matrix

---

## üìö Librerie principali

- `torch`, `torchvision`
- `transformers` (HuggingFace)
- `sklearn`
- `matplotlib`, `seaborn`

---

## üìà Risultati ottenuti

- **Accuracy finale su validation set**: ~99.7%
- **Loss** in costante diminuzione gi√† dalle prime epoche
- **Matrice di confusione** perfetta: tutte le 15 classi classificate senza errori nel validation set

---

## üìä Grafici

### Andamento della loss e accuratezza

<img width="1189" height="490" alt="download" src="https://github.com/user-attachments/assets/3686ba98-d75b-453f-995d-c69205253ae4" />

---

### Matrice di confusione

<img width="791" height="790" alt="download (1)" src="https://github.com/user-attachments/assets/6115c26e-41b1-4630-a0a9-d9629a5bc151" />



